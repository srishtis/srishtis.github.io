<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Srishti Saha</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="../img/favicon.png" rel="icon">
  <link href="../img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800|Montserrat:300,400,700" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="../lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="../lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="../lib/animate/animate.min.css" rel="stylesheet">
  <link href="../lib/ionicons/css/ionicons.min.css" rel="stylesheet">
  <link href="../lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="../lib/magnific-popup/magnific-popup.css" rel="stylesheet">
  <link href="../lib/ionicons/css/ionicons.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="../css/style.css" rel="stylesheet">

  <!-- =======================================================
    Theme Name: Reveal
    Theme URL: https://bootstrapmade.com/reveal-bootstrap-corporate-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body id="body">

  <!--==========================
    Top Bar
  ============================-->
  <section id="topbar" class="d-none d-lg-block">
    <div class="container clearfix">
      <div class="contact-info float-left">
        <i class="fa fa-envelope-o"></i> <a href="mailto:contact@example.com">srishtisaha.2809@gmail.com</a>
        <i class="fa fa-phone"></i> +1-2065811905
      </div>
      <div class="social-links float-right">
        <a href="https://www.linkedin.com/in/srishti-saha-b15975b8/" class="linkedin"><i class="fa fa-linkedin"></i></a>
		<a href="https://medium.com/@srishtisaha" class="medium"><i class="fa fa-medium"></i></a>
		<a href="https://github.com/srishtis" class="github"><i class="fa fa-github"></i></a>
		<a href="Resume_Srishti Saha_Feb2021.pdf" target="_blank"><i class="fa fa-file"></i></a>
      </div>
    </div>
  </section>

  <!--==========================
    Header
  ============================-->
  <header id="header">
    <div class="container">

      <div id="logo" class="pull-left">
        <h1><a href="../index.html" class="scrollto">Srishti Saha</a></h1>
        <!-- Uncomment below if you prefer to use an image logo -->
        <!-- <a href="#body"><img src="img/logo.png" alt="" title="" /></a>-->
      </div>

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li class="menu-active"><a href="#body">Home</a></li>
          <li><a href="projects/index.html">Projects</a></li>
          <li><a href="#sk_courses">Courses</a></li>
          <li class="menu-has-children"><a href="">Articles</a>
            <ul>
              <li><a href="https://www.yaabot.com/author/srishti-saha/">Yaabot (Technology)</a></li>
              <li><a href="https://blog.datahut.co/author/srishtisaha/">Datahut (Data Sciences and Analytics)</a></li>
              <li><a href="https://medium.com/@srishtisaha">Medium (Personal work)</a></li>
            </ul>
          </li>
          <li><a href="#contact">Contact</a></li>
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->


	
	<br> </br>

  <main id="main">

    <!--==========================
      Projects Section
    ============================-->

      <div class="container">
        <div class="section-header">
          <h4>Deep learning Approach for Question and Answering (QA) Systems</h4>
        </div>
		
		<h5>Skills Employed</h5>
		<ul>
			<li>Modelling Techniques: N-gram model, LSTM, LSTM+attention, Sequence-to-sequence, Memory Network</li>
			<li>Text Processing Techniques: Preprocessing (Lemmatization, Stop Word Removal, Removal of punctuation, numbers etc.), N-gram analysis</li>
			<li>Tech Stack: Python</li>
			<li>Libraries: Numpy, Pandas, Sklearn, NLTK, PyTorch</li>
		</ul>
		
		<h5>Github Link</h5>
		<p>The Github repository can be accessed from <a href="https://github.com/srishtis/DL_final_project_ece685">here.</a></p>

		<h5>Introduction</h5>
        <p> Question answering is one of the most challenging applications we have in Natural Language
		Processing. It is widely used in applications like information retrieval and entity extraction. It
		is also used as the back-end framework for systems like chatbots and for simulating human-like
		conversations. While it is ambitious to create and evaluate the performance of an agent in general
		dialogue, it is relatively easier to evaluate its responses to input questions. This enables us to test
		different capabilities of learning algorithms, under a common framework. Using QA frameworks,
		we aim to create a framework capable of open-domain question answering i.e. answering arbitrary
		questions with respect to arbitrary documents.

		Question answering systems enable users to retrieve exact answers for questions posed in natural
		language. They automatically answer questions asked by humans in natural language by referring to
		either a pre-structured database or a collection of documents/text-pieces written in a natural language.
		Question answering (QA) is a complex natural language processing (NLP) task. It requires an
		understanding of the meaning of a text piece (story) along with the ability to reason over relevant
		facts contained within the leading stories. It then employs a model that can answer questions based
		on logic, reasoning and even understanding of natural language.
		</p>

		<h5>Data</h5>
		<p>For building all the models in this report, we use the data from the Facebook bAbI project. bAbI
		is a carefully-designed set of 20 QA tasks. Each of these tasks consist of several context-questionanswer
		triplets, prepared and released by Facebook. Each task tests a different skill that a question
		answering model should have. The bAbI dataset comprises of synthetically generated stories about
		activity in a simulated world. This makes the vocabulary of this dataset very limited. Moreover, the
		sentence forms are very constrained. These limitations make this dataset an ideal dataset for rapid
		experiments and developing models. However, they raise questions about the ability to generalize
		results on bAbI to Question Answering scenarios in a less stringent/defined environment.
		The models are built with the intention that they perform well on the test data. For this purpose, we
		have split the given QA sets into train and test dataset. Even if a model shows a good performance
		here, it is not sufficient to conclude that the model would also exhibit this ability on real world text
		data.</p>
		
		<h5>Methodology</h5>
		<p>In this project, we employed 5 different models to form a QA system on the Facebook bAbI dataset. An overview of
		each model has been stated below.</p>
		
		<h5><u>N-gram model</u></h5>
		<p> An N-gram model constructs a sequence of N words that occur in a sequential combination in the
		given text. We used a N-gram model to define the baseline. We trained the N-gram model using both unigrams (1-gram) and bigrams (2-gram) individually
		to produce answers for the different tasks. For this, we selected only those sentences that had at
		least one common word with the story. We then constructed a bag-of-unigrams and a bigrams
		respectively from the stories. This was then fed into a Linear Support Vector Classification (SVC)
		model. We constructed two separate models using unigrams and bigrams respectively and selected
		the better-performing model.
		The linear classifier was used to predict the answers using the features extracted from the vocabulary
		of the stories. Since the model uses only the vocabulary as the features, it is predicted to perform
		sub-optimally in tasks that rely on reasoning beyond language.
		</p>
		
		<h5><u>LSTM</u></h5>
		<img src="lstm.PNG" />
		<br></br>
		<p> To capture the semantic structure of the stories and the question, we experimented with LSTM based
		sentence embedding. LSTM is a type of recurrent neural networks, that process the input word by
		word and update hidden state to capture the structure of the sequence of words in the sentence.
		The output of this model was a probability vector of the dimension of vocabulary.
		A graphical respresentation of the model has been given above.
		</p>
		
		<h5><u>Seq-2-Seq</u></h5>
		<img src="seq2seq.PNG" />
		<br></br>
		<p> To improve the performance of our models, we implemented a sequence-to-sequence model. The
		benefit of this framework over the other models is that it does not treat the answers as single-words
		and can thus incorporate comma-separated lists as answers, especially for tasks 8 (Lists/Sets) and 19
		(Path Finding). The figure above illustrates how a sequence-to-sequence network can be trained on a question
		answering task. The basic architecture of a seq-2-seq includes an RNN encoder (LSTM cell) that
		processes the stories (all concatenated together), followed by a special question-start symbol (SOQ),
		and then the question. The special SOS symbol tells the network to start decoding, with the decoderâ€™s
		initial state being the final state of the encoder. The decoder produces an answer sequence, followed
		by the special stop symbol EOS that indicates that processing must end. The network is trained using
		cross-entropy error on the decoder output, as compared with the correct answer sequence.
		During training, the decoder also receives the correct answer as input following the SOS symbol.
		During validation and testing, the correct answer is not provided: we only provide the SOS symbol.
		At subsequent steps, the output of time step <i>t</i> is fed to the decoder as the input at time step <i>t + 1</i>.
		</p>
		
		<h5><u>LSTM+attention mechanism</u></h5>
		<img src="lstmatt.PNG" />
		<br></br>
		<p> The LSTM model suffers in some tasks as the length of stories increases as facts mentioned earlier
		in the story suffer from diminishing gradient problem. The bAbI dataset has information about the
		specific stories required to answer the question. In this model we used this supporting information to
		pay attention to stories that are relevant. To achieve this we extended the LSTM model with attention
		mechanism.
		This model targets to learn two objectives. It tries to learn which story is relevant given the question,
		and given relevant stories it tries to learn to get closer to the answer. During training only the relevant
		sentences are fed as input into the previously described LSTM sentence embedding model. The
		mechanism is trained using the relevant sentence supervision signal provided with each example. For
		every story s, the model predicts the probability that the story is relevant in the example. During
		inference, any story with relevance probability greater than 0.5 was used to predict the answer. If no
		story had probability greater than 0.5 then the top two stories with highest probabilities were used.
		We used a joint loss function and optimized this loss using Adam optimizer. Since the vocabulary of
		the bAbI dataset is very small, we did not use any existing word embeddings and trained our own
		embeddings. The model is depicted graphically in the figure above.
		</p>
		
		<h5><u>End-to-end Memory Network</u></h5>
		<img src="memnw.PNG" />
		<br></br>
		<p> An end-to-end memory network, is a kind of memory network which uses
		simpler input feature maps and memory generalization steps than those used for dynamic memory
		networks. The simplification allowed for faster training and a greater range of experimentation
		within the scope of a course project. Furthermore, end-to-end networks have shown state-of-the
		art performance for weak supervision on the bAbI dataset. As we are interested in examining the
		greater generalizability of the weak supervision use case, end-to-end memory networks present a
		good choice for the main architecture for the project.
		</p>
		
		<h5>Results</h5>
		<p>
		A comprehensive table comparing the performance of each of these models is given below.
		<img src="results.PNG" />
		<br></br>
		</p>
		
		<h5>Impact</h5>
		<p>
		This project was executed as a part of our coursework at Duke Univeristy for the Deep Learning Course.
		</p>
		
		<h5>References</h5>
		<p>
		The following papers and literature were referred to, for execution and solution approach design.
		<ul>
			<li>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merrienboer,
			Armand Joulin, Tomas Mikolov. Towards AI-complete Question Answering : A Set of Prerequisite
			Toy Tasks. ICLR 2016</li>
			<li>Jason Weston, Sumit Chopra, Antoine Bordes. Memory Networks. ICLR 2015</li>
			<li>Ankit Kumar, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong,
			Romain Paulus, Richard Socher Ask Me Anything: Dynamic Memory Networks for Natural
			Language Processing.arXiv:1506.07285</li>
			</li>Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus. End-To-End Memory Networks.
			NIPS 2015.</li>
			<li>Eylon Stroh and Priyank Mathur. Question Answering Using Deep Learning. 2016.</li>
			<li>Fan Yang, Jiazhong Nie, William W. Cohen, Ni Lao Learning to Organize Knowledge and
			Answer Questions with N-Gram Machines. arXiv:1711.06744</li>
			<li>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa
			Suleyman, Phil Blunsom. Teaching Machines to Read and Comprehend.arXiv:1506.03340</li>
			<li>Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory.Neural computation, 9(8):
			1735â€“1780, 1997</li>
			<li>Matthew Richardson , Christopher J. C. Burges , Erin Renshaw. MCTest: A challenge dataset
			for the open-domain machine comprehension of text. EMNLP, pp. 193â€“203, 2013</li>
		</ul>
		</p>

      </div>



   <!--==========================
      Contact Section
    ============================-->
    <section id="contact" class="wow fadeInUp">
      <div class="container">
        <div class="section-header">
          <h2>Contact Me</h2>
        </div>

        <div class="row contact-info">

          <div class="col-md-4">
            <div class="contact-address">
              <i class="ion-ios-location-outline"></i>
              <h3>Address</h3>
              <address>2748 Campus Walk Ave, 20F, Durham-27705, NC, USA</address>
            </div>
          </div>

          <div class="col-md-4">
            <div class="contact-phone">
              <i class="ion-ios-telephone-outline"></i>
              <h3>Phone Number</h3>
              <p><a href="+918197346936">+1-2065811905</a></p>
            </div>
          </div>

          <div class="col-md-4">
            <div class="contact-email">
              <i class="ion-ios-email-outline"></i>
              <h3>Email</h3>
              <p><a href="mailto:srishti280992@yahoo.com">srishtisaha.2809@gmail.com</a></p>
            </div>
          </div>

        </div>
      </div>


      <div class="container">
        <div class="form">
          <div id="sendmessage">Your message has been sent. Thank you!</div>
          <div id="errormessage"></div>
          <form action="" method="post" role="form" class="contactForm">
            <div class="form-row">
              <div class="form-group col-md-6">
                <input type="text" name="name" class="form-control" id="name" placeholder="Your Name" data-rule="minlen:4" data-msg="Please enter at least 4 chars" />
                <div class="validation"></div>
              </div>
              <div class="form-group col-md-6">
                <input type="email" class="form-control" name="email" id="email" placeholder="Your Email" data-rule="email" data-msg="Please enter a valid email" />
                <div class="validation"></div>
              </div>
            </div>
            <div class="form-group">
              <input type="text" class="form-control" name="subject" id="subject" placeholder="Subject" data-rule="minlen:4" data-msg="Please enter at least 8 chars of subject" />
              <div class="validation"></div>
            </div>
            <div class="form-group">
              <textarea class="form-control" name="message" rows="5" data-rule="required" data-msg="Please write something for us" placeholder="Message"></textarea>
              <div class="validation"></div>
            </div>
            <div class="text-center"><button type="submit">Send Message</button></div>
          </form>
        </div>

      </div>
    </section><!-- #contact -->

  </main>

  <!--==========================
    Footer
  ============================-->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; <strong>Srishti Saha</strong>
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=Reveal
        -->
        Design by <a href="https://bootstrapmade.com/">BootstrapMade (Reveal)</a>
      </div>
    </div>
  </footer><!-- #footer -->

  <a href="#" class="back-to-top"><i class="fa fa-chevron-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="../lib/jquery/jquery.min.js"></script>
  <script src="../lib/jquery/jquery-migrate.min.js"></script>
  <script src="../lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../lib/easing/easing.min.js"></script>
  <script src="../lib/superfish/hoverIntent.js"></script>
  <script src="../lib/superfish/superfish.min.js"></script>
  <script src="../lib/wow/wow.min.js"></script>
  <script src="../lib/owlcarousel/owl.carousel.min.js"></script>
  <script src="../lib/magnific-popup/magnific-popup.min.js"></script>
  <script src="../lib/sticky/sticky.js"></script>
  <script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyD8HeI8o-c1NppZA-92oYlXakhDPYR7XMY"></script>
  <!-- Contact Form JavaScript File -->
  <script src="../contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="../js/main.js"></script>

</body>
</html>
